/**
 * Token chunker that splits text into fixed-size token chunks.
 */
import { Tokenizer } from './tokenizer.js';
import { Chunk } from './types.js';
export interface TokenChunkerOptions {
    /** Tokenizer instance or model name (default: 'character') */
    tokenizer?: Tokenizer | string;
    /** Maximum tokens per chunk (default: 512) */
    chunkSize?: number;
    /** Number of tokens to overlap between chunks (default: 0) */
    chunkOverlap?: number;
}
/**
 * Splits text into fixed-size token chunks with optional overlap.
 *
 * Uses character-based tokenization by default, but can use advanced
 * tokenizers from @chonkiejs/token package.
 */
export declare class TokenChunker {
    readonly chunkSize: number;
    readonly chunkOverlap: number;
    private tokenizer;
    private constructor();
    /**
     * Create a TokenChunker instance.
     *
     * @param options - Configuration options
     * @returns Promise resolving to TokenChunker instance
     *
     * @example
     * // Character-based (no dependencies)
     * const chunker = await TokenChunker.create({ chunkSize: 512 });
     *
     * @example
     * // With HuggingFace tokenizer (requires @chonkiejs/token)
     * const chunker = await TokenChunker.create({
     *   tokenizer: 'gpt2',
     *   chunkSize: 512,
     *   chunkOverlap: 50
     * });
     */
    static create(options?: TokenChunkerOptions): Promise<TokenChunker>;
    /**
     * Chunk a single text into fixed-size token chunks.
     *
     * @param text - The text to chunk
     * @returns Array of chunks
     */
    chunk(text: string): Promise<Chunk[]>;
    /**
     * Find the start index of chunk text in the original text.
     * This handles overlaps correctly.
     */
    private findStartIndex;
    toString(): string;
}
//# sourceMappingURL=token.d.ts.map
/**
 * Token chunker that splits text into fixed-size token chunks.
 */
import { Tokenizer } from './tokenizer.js';
import { Chunk } from './types.js';
/**
 * Splits text into fixed-size token chunks with optional overlap.
 *
 * Uses character-based tokenization by default, but can use advanced
 * tokenizers from @chonkiejs/token package.
 */
export class TokenChunker {
    constructor(tokenizer, chunkSize, chunkOverlap) {
        if (chunkSize <= 0) {
            throw new Error('chunkSize must be greater than 0');
        }
        if (chunkOverlap < 0) {
            throw new Error('chunkOverlap must be non-negative');
        }
        if (chunkOverlap >= chunkSize) {
            throw new Error('chunkOverlap must be less than chunkSize');
        }
        this.tokenizer = tokenizer;
        this.chunkSize = chunkSize;
        this.chunkOverlap = chunkOverlap;
    }
    /**
     * Create a TokenChunker instance.
     *
     * @param options - Configuration options
     * @returns Promise resolving to TokenChunker instance
     *
     * @example
     * // Character-based (no dependencies)
     * const chunker = await TokenChunker.create({ chunkSize: 512 });
     *
     * @example
     * // With HuggingFace tokenizer (requires @chonkiejs/token)
     * const chunker = await TokenChunker.create({
     *   tokenizer: 'gpt2',
     *   chunkSize: 512,
     *   chunkOverlap: 50
     * });
     */
    static async create(options = {}) {
        const { tokenizer = 'character', chunkSize = 512, chunkOverlap = 0, } = options;
        let tokenizerInstance;
        if (typeof tokenizer === 'string') {
            tokenizerInstance = await Tokenizer.create(tokenizer);
        }
        else {
            tokenizerInstance = tokenizer;
        }
        return new TokenChunker(tokenizerInstance, chunkSize, chunkOverlap);
    }
    /**
     * Chunk a single text into fixed-size token chunks.
     *
     * @param text - The text to chunk
     * @returns Array of chunks
     */
    async chunk(text) {
        if (!text) {
            return [];
        }
        const tokens = this.tokenizer.encode(text);
        const chunks = [];
        const step = this.chunkSize - this.chunkOverlap;
        for (let i = 0; i < tokens.length; i += step) {
            const chunkTokens = tokens.slice(i, i + this.chunkSize);
            const chunkText = this.tokenizer.decode(chunkTokens);
            const startIndex = this.findStartIndex(text, chunkText, i > 0 ? chunks[chunks.length - 1].endIndex : 0);
            const endIndex = startIndex + chunkText.length;
            chunks.push(new Chunk({
                text: chunkText,
                startIndex,
                endIndex,
                tokenCount: chunkTokens.length,
            }));
        }
        return chunks;
    }
    /**
     * Find the start index of chunk text in the original text.
     * This handles overlaps correctly.
     */
    findStartIndex(text, chunkText, searchFrom) {
        const index = text.indexOf(chunkText, searchFrom);
        return index !== -1 ? index : searchFrom;
    }
    toString() {
        return `TokenChunker(chunkSize=${this.chunkSize}, overlap=${this.chunkOverlap})`;
    }
}
//# sourceMappingURL=token.js.map
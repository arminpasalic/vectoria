/**
 * Simple character-based tokenizer for text chunking.
 *
 * This tokenizer treats each character as a single token, providing
 * a straightforward and predictable tokenization strategy.
 *
 * For advanced tokenization (GPT-2, BERT, etc.), use the static `create()` method
 * with @chonkiejs/token package installed.
 */
export declare class Tokenizer {
    /**
     * Create a tokenizer instance.
     *
     * @param model - Tokenizer model to use. Use 'character' (default) for character-based,
     *                or specify a HuggingFace model like 'gpt2', 'bert-base-uncased', etc.
     * @returns Promise resolving to a Tokenizer instance
     *
     * @example
     * // Character-based (no dependencies)
     * const tokenizer = await Tokenizer.create();
     * const tokenizer = await Tokenizer.create('character');
     *
     * @example
     * // HuggingFace models (requires @chonkiejs/token)
     * const tokenizer = await Tokenizer.create('gpt2');
     * const tokenizer = await Tokenizer.create('Xenova/gpt-4');
     */
    static create(model?: string): Promise<Tokenizer>;
    /**
     * Count the number of tokens in the given text.
     * For character-based tokenization, this is simply the length of the text.
     *
     * @param text - The text to count tokens for
     * @returns The number of tokens (characters) in the text
     */
    countTokens(text: string): number;
    /**
     * Encode text into token IDs.
     * For character-based tokenization, returns character codes.
     *
     * @param text - The text to encode
     * @returns Array of character codes
     */
    encode(text: string): number[];
    /**
     * Decode token IDs back into text.
     * For character-based tokenization, converts character codes back to string.
     *
     * @param tokens - Array of token IDs (character codes)
     * @returns The decoded text
     */
    decode(tokens: number[]): string;
    /**
     * Decode a batch of token arrays.
     *
     * @param tokensBatch - Array of token arrays
     * @returns Array of decoded texts
     */
    decodeBatch(tokensBatch: number[][]): string[];
}
//# sourceMappingURL=tokenizer.d.ts.map
import { Tokenizer } from './tokenizer.js';
import { Chunk, RecursiveRules } from './types.js';
/**
 * Recursively chunks text using a hierarchical set of rules.
 *
 * The chunker splits text at progressively finer granularities:
 * paragraphs → sentences → punctuation → words → characters
 *
 * Each chunk respects the configured chunk size limit.
 */
export class RecursiveChunker {
    constructor(tokenizer, chunkSize, rules, minCharactersPerChunk) {
        this.sep = '✄';
        this.CHARS_PER_TOKEN = 6.5;
        if (chunkSize <= 0) {
            throw new Error('chunkSize must be greater than 0');
        }
        if (minCharactersPerChunk <= 0) {
            throw new Error('minCharactersPerChunk must be greater than 0');
        }
        this.tokenizer = tokenizer;
        this.chunkSize = chunkSize;
        this.rules = rules;
        this.minCharactersPerChunk = minCharactersPerChunk;
    }
    /**
     * Create a RecursiveChunker instance.
     *
     * @param options - Configuration options
     * @returns Promise resolving to RecursiveChunker instance
     *
     * @example
     * // Character-based (no dependencies)
     * const chunker = await RecursiveChunker.create({ chunkSize: 512 });
     *
     * @example
     * // With HuggingFace tokenizer (requires @chonkiejs/token)
     * const chunker = await RecursiveChunker.create({
     *   tokenizer: 'gpt2',
     *   chunkSize: 512
     * });
     */
    static async create(options = {}) {
        const { tokenizer = 'character', chunkSize = 512, rules = new RecursiveRules(), minCharactersPerChunk = 24, } = options;
        let tokenizerInstance;
        if (typeof tokenizer === 'string') {
            tokenizerInstance = await Tokenizer.create(tokenizer);
        }
        else {
            tokenizerInstance = tokenizer;
        }
        return new RecursiveChunker(tokenizerInstance, chunkSize, rules, minCharactersPerChunk);
    }
    /**
     * Chunk a single text into an array of chunks.
     *
     * @param text - The text to chunk
     * @returns Array of chunks
     */
    async chunk(text) {
        return this.recursiveChunk(text, 0, 0);
    }
    /**
     * Estimate token count for a piece of text.
     * Uses a heuristic for quick estimation, falls back to actual counting.
     */
    async estimateTokenCount(text) {
        const estimate = Math.max(1, Math.floor(text.length / this.CHARS_PER_TOKEN));
        return estimate > this.chunkSize
            ? this.chunkSize + 1
            : this.tokenizer.countTokens(text);
    }
    /**
     * Split text according to a recursive level's rules.
     */
    async splitText(text, level) {
        // Whitespace splitting
        if (level.whitespace) {
            return text.split(' ');
        }
        // Delimiter splitting
        if (level.delimiters) {
            let processedText = text;
            const delims = Array.isArray(level.delimiters) ? level.delimiters : [level.delimiters];
            // Add separator based on includeDelim setting
            if (level.includeDelim === 'prev') {
                for (const delim of delims) {
                    processedText = processedText.replaceAll(delim, delim + this.sep);
                }
            }
            else if (level.includeDelim === 'next') {
                for (const delim of delims) {
                    processedText = processedText.replaceAll(delim, this.sep + delim);
                }
            }
            else {
                for (const delim of delims) {
                    processedText = processedText.replaceAll(delim, this.sep);
                }
            }
            const splits = processedText.split(this.sep).filter(s => s !== '');
            // Merge short splits
            const merged = [];
            let current = '';
            for (const split of splits) {
                if (split.length < this.minCharactersPerChunk) {
                    current += split;
                }
                else if (current) {
                    current += split;
                    merged.push(current);
                    current = '';
                }
                else {
                    merged.push(split);
                }
                if (current.length >= this.minCharactersPerChunk) {
                    merged.push(current);
                    current = '';
                }
            }
            if (current) {
                merged.push(current);
            }
            return merged;
        }
        // Token-based splitting (final level)
        const encoded = this.tokenizer.encode(text);
        const tokenSplits = [];
        for (let i = 0; i < encoded.length; i += this.chunkSize) {
            tokenSplits.push(encoded.slice(i, i + this.chunkSize));
        }
        return this.tokenizer.decodeBatch(tokenSplits);
    }
    /**
     * Create a chunk with proper metadata.
     */
    makeChunk(text, tokenCount, startOffset) {
        return new Chunk({
            text,
            startIndex: startOffset,
            endIndex: startOffset + text.length,
            tokenCount
        });
    }
    /**
     * Merge splits to respect chunk size limits.
     */
    mergeSplits(splits, tokenCounts, combineWhitespace = false) {
        if (!splits.length || !tokenCounts.length) {
            return [[], []];
        }
        if (splits.length !== tokenCounts.length) {
            throw new Error('Mismatch between splits and token counts');
        }
        // If all splits exceed chunk size, return as-is
        if (tokenCounts.every(count => count > this.chunkSize)) {
            return [splits, tokenCounts];
        }
        // Build cumulative token counts
        const cumulativeTokenCounts = [0];
        let sum = 0;
        for (const count of tokenCounts) {
            sum += count + (combineWhitespace ? 1 : 0);
            cumulativeTokenCounts.push(sum);
        }
        // Merge splits to fit chunk size
        const merged = [];
        const combinedTokenCounts = [];
        let currentIndex = 0;
        while (currentIndex < splits.length) {
            const currentTokenCount = cumulativeTokenCounts[currentIndex];
            const requiredTokenCount = currentTokenCount + this.chunkSize;
            let index = this.bisectLeft(cumulativeTokenCounts, requiredTokenCount, currentIndex) - 1;
            index = Math.min(index, splits.length);
            if (index === currentIndex) {
                index += 1;
            }
            // Merge splits
            if (combineWhitespace) {
                merged.push(splits.slice(currentIndex, index).join(' '));
            }
            else {
                merged.push(splits.slice(currentIndex, index).join(''));
            }
            combinedTokenCounts.push(cumulativeTokenCounts[Math.min(index, splits.length)] - currentTokenCount);
            currentIndex = index;
        }
        return [merged, combinedTokenCounts];
    }
    /**
     * Binary search helper for merging splits.
     */
    bisectLeft(arr, value, lo = 0) {
        let hi = arr.length;
        while (lo < hi) {
            const mid = (lo + hi) >>> 1;
            if (arr[mid] < value) {
                lo = mid + 1;
            }
            else {
                hi = mid;
            }
        }
        return lo;
    }
    /**
     * Core recursive chunking logic.
     */
    async recursiveChunk(text, level, startOffset) {
        if (!text) {
            return [];
        }
        // Base case: no more levels
        if (level >= this.rules.length) {
            const tokenCount = await this.estimateTokenCount(text);
            return [this.makeChunk(text, tokenCount, startOffset)];
        }
        const currRule = this.rules.getLevel(level);
        if (!currRule) {
            throw new Error(`No rule found at level ${level}`);
        }
        // Split according to current level's rules
        const splits = await this.splitText(text, currRule);
        const tokenCounts = await Promise.all(splits.map(split => this.estimateTokenCount(split)));
        // Merge splits based on level type
        let merged;
        let combinedTokenCounts;
        if (currRule.delimiters === undefined && !currRule.whitespace) {
            // Token level - no merging
            [merged, combinedTokenCounts] = [splits, tokenCounts];
        }
        else if (currRule.delimiters === undefined && currRule.whitespace) {
            // Whitespace level - merge with spaces
            [merged, combinedTokenCounts] = this.mergeSplits(splits, tokenCounts, true);
            // Add space prefix to all but first split
            merged = merged.slice(0, 1).concat(merged.slice(1).map(t => ' ' + t));
        }
        else {
            // Delimiter level - merge without spaces
            [merged, combinedTokenCounts] = this.mergeSplits(splits, tokenCounts, false);
        }
        // Recursively process merged splits
        const chunks = [];
        let currentOffset = startOffset;
        for (let i = 0; i < merged.length; i++) {
            const split = merged[i];
            const tokenCount = combinedTokenCounts[i];
            if (tokenCount > this.chunkSize) {
                // Recursively chunk oversized splits
                chunks.push(...await this.recursiveChunk(split, level + 1, currentOffset));
            }
            else {
                chunks.push(this.makeChunk(split, tokenCount, currentOffset));
            }
            currentOffset += split.length;
        }
        return chunks;
    }
    toString() {
        return `RecursiveChunker(chunkSize=${this.chunkSize}, levels=${this.rules.length})`;
    }
}
//# sourceMappingURL=recursive.js.map